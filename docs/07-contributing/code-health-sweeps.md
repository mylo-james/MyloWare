# Code Health Sweeps: Coverage + AI Bloat

**Audience:** Human + AI agents doing iterative quality passes  
**Outcome:** Every file is scanned for AI bloat, behaviours are covered by tests, and each sweep leaves reusable evidence.

This protocol is meant to be run over and over by different agents as we move from v0.9 → v1.0. It assumes you have read:

- `docs/development-cycle.md`
- `docs/07-contributing/testing.md`

We follow the same cycle: **Observe → Diagnose → Fix → Verify → Document**, but focused on coverage + AI bloat.

---

## 0. One‑Time Setup (per machine)

- Be in the repo root with a working venv.
- Ensure the local stack can run:
  - `docker compose -f infra/docker-compose.yml up -d`
- Ensure tests and coverage work:
  - `make test-coverage` (writes `coverage.json` + `python-coverage.xml`).
- Make sure these files exist (create empty if missing):
  - `state-of-the-repo.md`
  - `flagged-bloat.md`

From here, each **sweep iteration** is independent and can be picked up by any agent.

---

## 1. Artifacts This Protocol Maintains

Every sweep should update or create the following:

- `coverage.json` / `python-coverage.xml`  
  Single source of truth for coverage, regenerated by `make test-coverage`.

- `evidence/file_scan_registry.json`  
  Registry of which files have been scanned for AI bloat at least once.  
  Suggested shape:
  ```json
  {
    "iteration_id": "2025-11-21T12-34-56Z",
    "files": {
      "apps/orchestrator/persona_nodes.py": {
        "first_scanned_at": "2025-11-21T12-34-56Z",
        "last_scanned_at": "2025-11-21T12-34-56Z",
        "scan_count": 1
      }
    }
  }
  ```
  Agents can extend this structure, but should not remove entries.

- `flagged-bloat.md`  
  Human‑readable log of AI bloat issues (see format below). This is the canonical list of bloat we have **identified**, with status (`open`, `accepted`, `fixed`).

- `state-of-the-repo.md`  
  Rolling changelog of important runs and coverage state. Add a short entry per sweep iteration.

- Optional per‑iteration artifact  
  For deeper automation, agents may create `evidence/coverage_iteration_<iteration_id>.json` with:
  - `iteration_id`
  - `git_sha`
  - `coverage_before` / `coverage_after` (overall + per‑target-file)
  - `target_files`
  - `e2e_smoke_run_ids`

---

## 2. Sweep Iteration Overview

Each iteration does:

1. Run coverage and select target files.  
2. For each target file: map behaviour and scan for AI bloat.  
3. Design tests to cover behaviours.  
4. Implement tests and, optionally, small bloat refactors.  
5. Re‑run coverage + a smoke E2E, then log the iteration.

Different agents can own different iterations. The only contract is that they update the shared artifacts above.

---

## 3. Step 1 – Run Coverage & Select Target Files

**Goal:** Choose a small, deterministic set of files to improve, while eventually touching **every** file.

1. Run coverage:
   ```bash
   make test-coverage
   ```
   This regenerates `coverage.json` and `python-coverage.xml`.

2. Build a candidate list of files:
   - Include all non‑test Python files under:
     - `apps/api`
     - `apps/orchestrator`
     - `adapters`
     - `cli`
     - `content`
     - `core`
   - Exclude:
     - `tests/**`
     - Alembic migrations and auto‑generated files
     - Trivial `__init__.py` that only re‑export symbols

3. Use `evidence/file_scan_registry.json` to separate:
   - **Unscanned files** (not present in the registry).
   - **Scanned files** (present, with `scan_count` ≥ 1).

4. Select target files for this iteration:
   - Fill the target list with up to 5 files:
     1. Prefer unscanned files first (to eventually cover the whole tree).
     2. If there are more than 5 unscanned files, choose the **5 with lowest coverage**.
     3. If there are fewer than 5 unscanned files, fill remaining slots with **lowest‑coverage scanned files** that were not touched in the last 3 iterations.
   - Ensure diversity when possible:
     - At least 1 from `apps/api` or `apps/orchestrator`.
     - At least 1 from `adapters/**`.
     - At least 1 from CLI/content/core.

5. Update `evidence/file_scan_registry.json`:
   - For each selected file:
     - If new: add an entry with `first_scanned_at`, `last_scanned_at`, `scan_count = 1`.
     - If existing: update `last_scanned_at` and increment `scan_count`.

6. (Optional) Write an iteration artifact under `evidence/` describing:
   - `iteration_id` (timestamp or short slug).
   - `target_files` with their current coverage.
   - `coverage_before` snapshot (overall + per‑package).

---

## 4. Step 2 – Behaviour Mapping + AI Bloat Scan

**Goal:** Understand what each target file is supposed to do, and identify AI bloat while you are there.

For each target file:

1. Map behaviour in 3–5 bullets:
   - What responsibilities does this file own?
   - What are the main public functions/classes?
   - Which external systems does it talk to (DB, providers, orchestrator graph, CLI)?

2. Scan for **AI bloat** (flag, don’t fix yet):
   - Look for:
     - Direct LLM calls buried in endpoints or persona nodes instead of going through provider adapters/tools.
     - Large or repetitive prompt templates that could be:
       - Centralised in shared templates.
       - Replaced by simple, deterministic logic.
     - Multiple sequential LLM calls where a pure function or small state machine would work.
     - Unbounded context concatenation (dumping large artifacts into prompts without filtering/truncation).
     - “Roleplay” instructions and narrative that do not affect behaviour.
   - Assign one or more categories:
     - `prompt-bloat`
     - `llm-instead-of-code`
     - `unbounded-context`
     - `duplicate-policy-prompt`
     - `other` (with explanation)

3. Append an entry to `flagged-bloat.md` (see Section 6) for **each distinct issue**, not just per file.

4. If no bloat is found:
   - Add a line to `flagged-bloat.md` noting that the file was scanned with **no AI bloat** detected, so we know it’s been reviewed.

---

## 5. Step 3 – Test Design (Per File)

**Goal:** Decide what to test before touching any code.

For each target file:

1. Identify critical behaviours:
   - At least:
     - One happy‑path behaviour.
     - One or two edge cases (bad inputs, provider error, configuration issues).

2. Map behaviours to test types:
   - Pure logic:
     - Unit tests under `tests/unit/**` in the appropriate package.
   - Orchestrator nodes / persona tools:
     - Unit tests around the node or tool.
     - Integration tests in `tests/integration/python_orchestrator/**` when graph wiring matters.
   - Adapters / provider clients:
     - Unit tests with mocked HTTP, covering:
       - Happy path.
       - Timeout / non‑200.
       - Malformed payloads.
   - API endpoints:
     - Tests in `tests/integration/python_api/**` using the FastAPI test client.

3. Write a short test plan in your iteration notes (or in the per‑iteration artifact), for example:
   - `apps/orchestrator/persona_nodes.py`
     - `test_riley_node_calls_expected_tools_happy_path`
     - `test_riley_node_fails_fast_when_expectations_missing`

---

## 6. Step 4 – Implement Tests + Optional Bloat Refactors

**Goal:** Raise coverage in a safe, incremental way and only refactor bloat once behaviour is pinned by tests.

1. Implement the tests from Step 3:
   - Follow `docs/07-contributing/testing.md` for structure and tools.
   - Keep tests deterministic; mock providers at adapter boundaries.
   - Prefer fixtures/fakes instead of heavy patching where possible.

2. Re‑run the relevant subset first:
   ```bash
   pytest -q tests/unit/...  # narrow path for fast feedback
   ```

3. Optional: Apply small AI‑bloat refactors when safe:
   - Only after tests are in place or added in the same change.
   - Examples:
     - Extract duplicated prompt text into a shared constant/template.
     - Replace trivial “LLM as formatter” calls with pure Python helpers.
     - Add truncation/selection when prompts include large histories.
   - Keep changes small and fail‑fast (no silent fallbacks).

4. Update `flagged-bloat.md`:
   - For each issue you changed, update its status to `fixed` and link to:
     - PR or commit SHA.
     - Any relevant evidence (run IDs, LangSmith traces).

---

## 7. Step 5 – Verify & Document the Sweep

**Goal:** Prove that coverage improved (or at least didn’t regress), E2E behaviour still works, and the sweep is auditable.

1. Run the full test suite with coverage:
   ```bash
   make test-coverage
   ```

2. Capture key numbers:
   - Overall coverage before vs after.
   - Per‑package coverage (`apps/api`, `apps/orchestrator`, `adapters`, etc.).
   - Coverage for each target file.

3. Run at least one E2E smoke:
   - Kick off a run for `aismr` or `test_video_gen` via:
     ```bash
     curl -X POST https://myloware-api-staging.fly.dev/v1/runs/start \
       -H "x-api-key: $API_KEY" \
       -d '{"project": "test_video_gen", "input": "Generate test videos"}'
     ```
   - Record the `runId`, confirm the run completes and uses expected tools.

4. Append an entry to `state-of-the-repo.md`:
   - `iteration_id`, date, git SHA.
   - Coverage before → after (overall + per‑package).
   - List of target files.
   - Number of AI‑bloat issues:
     - Newly logged.
     - Fixed in this iteration.
   - E2E `runId`s and status.

5. Commit updated artifacts:
   - `evidence/file_scan_registry.json`
   - `flagged-bloat.md`
   - Any new/updated tests
   - `state-of-the-repo.md`

---

## 8. `flagged-bloat.md` Format

`flagged-bloat.md` lives at the repo root. It is a **global index** of AI bloat findings and their status.

Recommended structure:

```markdown
# AI Bloat Findings

> One section per file, one bullet per distinct issue.

## apps/orchestrator/persona_nodes.py

- [open] `prompt-bloat` — Riley system prompt repeats repo‑wide policies; extract to shared template.  
  - Reported in iteration: `2025-11-21T12-34-56Z` by @agent‑name  
  - Evidence: run `abcd1234`, LangSmith trace `...`

- [fixed] `llm-instead-of-code` — Used LLM for simple slug formatting; replaced with pure Python helper.  
  - Fixed in PR `#123`, commit `abcd1234`

## adapters/ai_providers/shotstack/client.py

- [no-bloat] Scanned in iteration `2025-11-21T12-34-56Z` — no AI bloat identified.
```

Agents can add extra metadata (links, tags), but should keep the basic structure consistent so future agents and scripts can parse it.

---

## 9. Repeat Forever (with Guards)

To keep the protocol sustainable:

- Do not re‑target the same file in back‑to‑back iterations unless:
  - Coverage regressed, or
  - A new AI‑bloat issue was reported there.

- Once a file:
  - Has been scanned at least once, and
  - Has coverage ≥ 90 %, and
  - Has no open AI‑bloat issues in `flagged-bloat.md`,
  - …treat it as **green**. Only re‑visit if its behaviour changes.

- If coverage drops or E2E smokes fail in an iteration:
  - Mark that iteration as **red** in `state-of-the-repo.md`.
  - Prioritise regression diagnosis in the next iteration.

Run this protocol until:

- All relevant files appear in `evidence/file_scan_registry.json`.
- All critical behaviours have tests.
- `flagged-bloat.md` has no high‑priority open items.

At that point, we are much closer to a real **v1.0**.

