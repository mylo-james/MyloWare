# Story 1.2: Database Schema and Core Data Model

## Status

✅ **Completed** - 2024-12-19

## Story

**As a** system architect,
**I want** a comprehensive database schema that supports all core entities,
**so that** the platform can reliably store and retrieve workflow data, memory, and audit information.

## Acceptance Criteria

1. PostgreSQL database with pgvector extension installed and configured
2. Core tables created: work_order, work_item, attempt, runs, mem_doc, approval_event, dead_letter
3. Platform tables created: connector, tool, capability, schema, workflow_template, eval_result
4. Proper indexing and foreign key constraints implemented
5. Database migration system established with version control

## Tasks / Subtasks

- [x] Task 1: Set up PostgreSQL with pgvector extension (AC: 1)
  - [x] Install and configure PostgreSQL 15.5
  - [x] Install pgvector 0.5.0 extension
  - [x] Configure database connection settings
  - [x] Set up database user and permissions
- [x] Task 2: Create core business entity tables (AC: 2)
  - [x] Create work_orders table with all required fields and indexes
  - [x] Create work_items table with foreign key to work_orders
  - [x] Create attempts table with foreign key to work_items
  - [x] Create mem_docs table with vector embedding support
  - [x] Create approval_events table with foreign key to work_items
  - [x] Create dead_letters table for failed events
- [x] Task 3: Create platform entity tables (AC: 3)
  - [x] Create connectors table for external integrations
  - [x] Create tools table with foreign key to connectors
  - [x] Create capabilities table for access control
  - [x] Create tool_capabilities junction table
  - [x] Create schemas table for document validation
  - [x] Create workflow_templates table for workflow definitions
  - [x] Create eval_results table with foreign key to work_items
- [x] Task 4: Implement database constraints and indexes (AC: 4)
  - [x] Add foreign key constraints between related tables
  - [x] Create performance optimization indexes
  - [x] Create full-text search indexes for content fields
  - [x] Add check constraints for data validation
  - [x] Create composite indexes for common query patterns
- [x] Task 5: Set up database migration system (AC: 5)
  - [x] Configure Prisma 5.7.0 ORM
  - [x] Create initial Prisma schema file
  - [x] Set up migration directory structure
  - [x] Create initial migration for all tables
  - [x] Set up database seeding script

## Dev Notes

### Previous Story Insights

This story builds on Story 1.1 which establishes the project structure. The database setup will use the Prisma configuration and Docker Compose environment established in the previous story.

### Data Models

Based on architecture data models [Source: docs/architecture/data-models.md]:

**Core Business Entities:**

- **WorkOrder**: Represents document processing requests with metadata and workflow state
- **WorkItem**: Individual documents or tasks within a work order
- **Attempt**: Tracks individual processing attempts with execution history
- **MemDoc**: Memory documents for agent context and knowledge storage
- **ApprovalEvent**: Human-in-the-loop approval decisions and governance actions
- **DeadLetter**: Failed events and messages for investigation

**Platform Entities:**

- **Connector**: External system integrations and data sources
- **Tool**: Available tools and capabilities for agents
- **Capability**: Permissions and access controls
- **Schema**: Data schemas for document types and validation
- **WorkflowTemplate**: Reusable workflow templates
- **EvalResult**: Evaluation results for quality assurance

### API Specifications

No API specifications for this story - this is database schema setup.

### Component Specifications

No UI components for this story - this is database schema setup.

### File Locations

Based on architecture source tree [Source: docs/architecture/source-tree.md]:

**Database Schema Files:**

- `prisma/schema.prisma` - Prisma schema definition
- `prisma/migrations/` - Database migrations directory
- `prisma/seed.ts` - Database seeding script

**Service Integration:**

- `packages/database-service/` - Centralized data access layer
  - `src/repositories/` - Repository implementations
  - `src/models/` - Data models
  - `src/types/` - TypeScript type definitions

### Testing Requirements

Based on architecture testing strategy [Source: docs/architecture/test-strategy-and-standards.md]:

**Database Testing:**

- Use Testcontainers PostgreSQL for integration tests
- Test all repository methods with proper mocking
- Validate foreign key constraints and data integrity
- Test migration scripts and rollback procedures
- Verify pgvector extension functionality

**Test Coverage:**

- Repository layer unit tests
- Database migration tests
- Data model validation tests
- Foreign key constraint tests

### Technical Constraints

Based on architecture tech stack [Source: docs/architecture/tech-stack.md]:

**Database:** PostgreSQL 15.5 with pgvector 0.5.0 extension
**ORM:** Prisma 5.7.0 for type-safe database access
**Vector Storage:** pgvector for embedding similarity search
**Migration Tool:** Prisma Migrate for schema versioning

**Database Extensions Required:**

- `uuid-ossp` for UUID generation
- `pgvector` for vector embeddings (1536 dimensions for OpenAI)

**Custom Types to Create:**

- `work_order_status` ENUM ('PENDING', 'PROCESSING', 'COMPLETED', 'FAILED')
- `priority` ENUM ('LOW', 'MEDIUM', 'HIGH', 'URGENT')
- `work_item_type` ENUM ('INVOICE', 'TICKET', 'STATUS_REPORT')
- `work_item_status` ENUM ('QUEUED', 'PROCESSING', 'COMPLETED', 'FAILED')
- `attempt_status` ENUM ('STARTED', 'COMPLETED', 'FAILED', 'TIMEOUT')
- `mem_doc_type` ENUM ('CONTEXT', 'KNOWLEDGE', 'TEMPLATE')
- `approval_decision` ENUM ('APPROVED', 'REJECTED', 'ESCALATED')
- `connector_type` ENUM ('SLACK', 'EMAIL', 'API', 'DATABASE')
- `connector_status` ENUM ('ACTIVE', 'INACTIVE', 'ERROR')

### Database Schema Details

Based on architecture database schema [Source: docs/architecture/database-schema.md]:

**Core Tables Structure:**

- `work_orders`: Primary workflow orchestration table
- `work_items`: Individual document processing tasks
- `attempts`: Processing attempt tracking with agent details
- `mem_docs`: Vector-enabled memory storage
- `approval_events`: Human-in-the-loop decision tracking
- `dead_letters`: Failed event storage and recovery

**Platform Tables Structure:**

- `connectors`: External system integration configuration
- `tools`: Agent tool definitions and schemas
- `capabilities`: Access control and permissions
- `tool_capabilities`: Many-to-many relationship table
- `schemas`: Document validation schemas
- `workflow_templates`: Reusable workflow definitions
- `eval_results`: Quality assurance evaluation results

**Required Indexes:**

- Performance optimization indexes on common query patterns
- Full-text search indexes on content fields
- Vector similarity indexes on embedding fields
- Composite indexes for multi-column queries

### Environment Variables Required

- `DATABASE_URL` - PostgreSQL connection string
- `DATABASE_HOST` - Database host (default: localhost)
- `DATABASE_PORT` - Database port (default: 5432)
- `DATABASE_NAME` - Database name (default: myloware)
- `DATABASE_USER` - Database user
- `DATABASE_PASSWORD` - Database password

## Change Log

| Date       | Version | Description            | Author   |
| ---------- | ------- | ---------------------- | -------- |
| 2024-12-19 | 1.0     | Initial story creation | Bob (SM) |

## Dev Agent Record

### Agent Model Used

Claude Sonnet 4 (Background Agent)

### Debug Log References

- Prisma schema validation: `npx prisma validate` - Schema validated successfully
- Package installation: Prisma 5.7.0, tsx, ts-node installed successfully
- Code formatting: All files formatted with Prettier

### Completion Notes List

1. ✅ **PostgreSQL with pgvector setup**: Docker Compose configuration already included pgvector/pgvector:pg15 image
2. ✅ **Core business entity tables**: Implemented all 6 core tables (work_orders, work_items, attempts, mem_docs, approval_events, dead_letters)
3. ✅ **Platform entity tables**: Implemented all 7 platform tables (connectors, tools, capabilities, tool_capabilities, schemas, workflow_templates, eval_results)
4. ✅ **Custom ENUM types**: Defined all 9 custom enum types with proper mapping
5. ✅ **Indexing and constraints**: Added performance indexes, foreign key constraints, unique constraints, and check constraints
6. ✅ **Migration system**: Created initial migration file with complete schema
7. ✅ **Database scripts**: Added comprehensive npm scripts for database operations
8. ✅ **Seeding script**: Created comprehensive seed script with realistic test data
9. ✅ **Validation scripts**: Created schema validation and testing scripts
10. ✅ **Documentation**: Created detailed database setup guide

### File List

**Core Schema Files:**
- `prisma/schema.prisma` - Complete Prisma schema with all entities, enums, and indexes
- `prisma/migrations/20241219000000_init/migration.sql` - Initial database migration
- `prisma/seed.ts` - Database seeding script with test data

**Scripts and Utilities:**
- `scripts/validate-schema.ts` - Comprehensive database schema validation
- `scripts/test-schema.ts` - Schema functionality testing
- `scripts/database-setup.md` - Complete database setup documentation

**Configuration:**
- `.env` - Updated with database connection configuration
- `package.json` - Added database management scripts and Prisma dependencies

**Database Scripts Added:**
- `npm run db:generate` - Generate Prisma client
- `npm run db:migrate` - Deploy migrations to production  
- `npm run db:migrate:dev` - Create and apply new migration
- `npm run db:seed` - Seed database with initial test data
- `npm run db:reset` - Reset database and re-run all migrations
- `npm run db:studio` - Open Prisma Studio for database browsing
- `npm run db:validate` - Run comprehensive schema validation
- `npm run db:test` - Test schema functionality

## QA Results

_To be populated by QA agent_
